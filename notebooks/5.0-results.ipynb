{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ Analysis\n",
    "\n",
    "This notebook analysis the experimental results from training a digital-to-film style transfer model. It uses the experiment results that are tracked to W&B during training from the `scr/train.py` script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "Let's install some necessary dependencies and set global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import wandb\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WANDB_PROJECT = \"sillystill\"\n",
    "WANDB_ENTITY = \"sillystill\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to W&B\n",
    "\n",
    "---\n",
    "\n",
    "Let's start by loading all the runs from the W&B project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get runs\n",
    "runs = api.runs(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
    "print(f\"âœ… Loaded {len(runs)} runs from W&B ({WANDB_ENTITY}/{WANDB_PROJECT})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Single Image\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_results(path, runs):\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    baselines_cols = [\"baseline_lpips\", \"baseline_psnr\", \"baseline_ssim\", \"baseline_pieapp\"]\n",
    "    baselines = data.iloc[0][baselines_cols].rename(lambda x: x.replace(\"baseline_\", \"\"), axis=0)\n",
    "\n",
    "    # Remove baseline cols  + media col\n",
    "    data = data.drop(columns=[\"digitalfilepath\", \"filmfilepath\", \"predictedfilepath\"])\n",
    "    data = data.drop(columns=[\"baseline_lpips\", \"baseline_psnr\", \"baseline_ssim\", \"baseline_pieapp\"])\n",
    "\n",
    "    # Retrieve other runs from api\n",
    "    runs = [run for run in runs if run.name in data[\"run_name\"].unique()]\n",
    "\n",
    "    tags = [\" - \".join(set(run.tags) - set([\"Combined\"])) for run in runs]\n",
    "    with_noise = [run.config[\"model\"][\"net\"][\"with_noise\"] for run in runs]\n",
    "    data[\"tags\"] = tags\n",
    "    data[\"with_noise\"] = with_noise\n",
    "\n",
    "    # Add baseline\n",
    "    idx = len(data)\n",
    "    data.loc[idx] = baselines\n",
    "    data.loc[idx, \"run_name\"] = \"baseline\"\n",
    "    data.loc[idx, \"tags\"] = \"Baseline\"\n",
    "    data.loc[idx, \"with_noise\"] = 0.0\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image_raw = load_single_results(\"outputs/single-image.csv\", runs)\n",
    "\n",
    "# Split tags into resize and loss column\n",
    "single_image_raw[\"resize\"] = single_image_raw[\"tags\"].apply(lambda x: int(\"Resized\" in x))\n",
    "single_image_raw[\"loss\"] = single_image_raw[\"tags\"].apply(lambda x: x.replace(\"- Resized\", \"\").strip())\n",
    "\n",
    "# Make noise column boolean\n",
    "single_image_raw[\"with_noise\"] = single_image_raw[\"with_noise\"].astype(int)\n",
    "\n",
    "# Sort by loss\n",
    "single_image = single_image_raw.drop(columns=[\"tags\"])\n",
    "single_image.sort_values(by=\"ssim\", inplace=True, ascending=False)\n",
    "single_image[single_image['resize'] == 1].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image[single_image['loss'].isin([\"MSE\", \"MAE\"])].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple plot for losses with no resizing and no noise\n",
    "\n",
    "def plot_losses(data, title, y_value, x_label, with_noise=0, resize=0, save_path=None, ylim = None, cutoff=0):\n",
    "    data = data[data[\"resize\"] == resize]\n",
    "    data = data[data[\"with_noise\"] == with_noise]\n",
    "    data = data.drop(columns=[\"resize\", \"with_noise\"])\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    sns.set_context(\"notebook\", font_scale=1.5)\n",
    "    sns.set_palette(\"colorblind\")\n",
    "\n",
    "    # Remove bottom 3 performing models\n",
    "    data = data.iloc[cutoff:]\n",
    "\n",
    "    ax = sns.barplot(data=data, x=\"loss\", y=y_value, hue=\"run_name\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_value.upper())\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.legend().remove()\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(single_image, \"Single Image Losses\", \"pieapp\", \"Loss\", with_noise=0, cutoff=1) # \"outputs/single-image-losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do some plotting here\n",
    "\n",
    "# Probably useful to pivot the data\n",
    "# single_image.melt(id_vars=[\"run_name\", \"tags\", \"with_noise\"], var_name=\"metric\", value_name=\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Full Data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_results(path, runs):\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    data = data.drop(columns=[\"digitalfilepath\", \"filmfilepath\", \"predictedfilepath\"])\n",
    "\n",
    "    # Make index from 1-37 replicated num_runs times\n",
    "    idx = list(range(1, 38)) * data.run_name.nunique()\n",
    "    data[\"image_idx\"] = idx\n",
    "\n",
    "    # Add run names\n",
    "    runs = [run for run in runs if run.name in data[\"run_name\"].unique()]\n",
    "    tags = pd.Series([\" - \".join(set(run.tags) - set([\"Combined\"])) for run in runs]).repeat(37).reset_index(drop=True)\n",
    "    with_noise = pd.Series([run.config[\"model\"][\"net\"][\"with_noise\"] for run in runs]).repeat(37).reset_index(drop=True)\n",
    "    data[\"tags\"] = tags\n",
    "    data[\"with_noise\"] = with_noise\n",
    "\n",
    "    # Grab baselines\n",
    "    baselines_cols = [\"baseline_lpips\", \"baseline_psnr\", \"baseline_ssim\", \"baseline_pieapp\"]\n",
    "    baselines = data.iloc[:37, 5:-1].rename(lambda x: x.replace(\"baseline_\", \"\"), axis=1)\n",
    "    baselines[\"image_idx\"] = idx[:37]\n",
    "    baselines[\"run_name\"] = \"baseline\"\n",
    "    baselines[\"tags\"] = \"Baseline\"\n",
    "    baselines[\"with_noise\"] = False\n",
    "    data = data.drop(columns=baselines_cols)\n",
    "\n",
    "    return pd.concat([data, baselines], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_raw = load_full_results(\"outputs/full-data.csv\", runs)\n",
    "# Split tags into resize and loss column\n",
    "full_data_raw[\"resize\"] = full_data_raw[\"tags\"].apply(lambda x: int(\"Resized\" in x))\n",
    "full_data_raw[\"loss\"] = full_data_raw[\"tags\"].apply(lambda x: x.replace(\"- Resized\", \"\").strip())\n",
    "\n",
    "# Make noise column boolean\n",
    "full_data_raw[\"with_noise\"] = full_data_raw[\"with_noise\"].astype(int)\n",
    "\n",
    "# Sort by loss\n",
    "full_data = full_data_raw.drop(columns=[\"tags\"])\n",
    "full_data.sort_values(by=\"ssim\", inplace=True)\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mean and std for each metric\n",
    "# With std\n",
    "# grouped_full_data = full_data.groupby([\"run_name\", \"loss\", \"resize\", \"with_noise\"]).agg(\n",
    "#     {\"lpips\": [\"mean\", \"std\"],\n",
    "#      \"psnr\": [\"mean\", \"std\"],\n",
    "#      \"ssim\": [\"mean\", \"std\"],\n",
    "#      \"pieapp\": [\"mean\", \"std\"]}).sort_values(by=(\"ssim\", \"mean\"), ascending=False)\n",
    "\n",
    "\n",
    "# Only mean\n",
    "grouped_full_data = full_data.groupby([\"run_name\", \"loss\", \"resize\", \"with_noise\"]).agg(\n",
    "    {\"lpips\": \"mean\",\n",
    "     \"psnr\": \"mean\",\n",
    "     \"ssim\": \"mean\",\n",
    "     \"pieapp\": \"mean\"}).sort_values(by=\"ssim\", ascending=False).reset_index()\n",
    "grouped_full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(grouped_full_data, \"Full Data Losses\", \"psnr\", \"loss\", with_noise=0, resize=0) # \"outputs/full-data-losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do some plotting here\n",
    "\n",
    "# Probably useful to pivot the data\n",
    "# full_data.melt(id_vars=[\"run_name\", \"tags\", \"with_noise\"], var_name=\"metric\", value_name=\"value\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sillystill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
