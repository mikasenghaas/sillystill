{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¸ Data Preprocessing\n",
    "\n",
    "This notebook details the steps taken to preprocess the data for the project. The raw image pairs are stored in the folder `data/raw` and the cleaned data will be stored in the folder `data/processed`. Each sample is a pair of images that capture the same scence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "Let's install some necessary dependencies and set global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoroot\n",
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Imports\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment (Dummy Images)\n",
    "\n",
    "---\n",
    "\n",
    "Due to the different setup in the cameras (different lenses with different focal lengths), the images for an image pair are not exactly aligned. For this reason, we have to come up with an automated way of aligning the images. A classical computer vision pipeline to achieve this is the following:\n",
    "\n",
    "* Extract meaningful features from both images (called **keypoints**)\n",
    "* Match the features from both images\n",
    "* Transform, warp and crop the larger image for alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image example\n",
    "train = cv.imread(\"imgs/cinestill-800t.jpg\")\n",
    "train = cv.cvtColor(train, cv.COLOR_BGR2RGB)  # from BGR to RGB\n",
    "\n",
    "# Apply some basic transformations (crop + rotation)\n",
    "height, width = train.shape[:2]\n",
    "M = cv.getRotationMatrix2D(((width // 2, height // 2)), 10, 1.0)\n",
    "query = rotated = cv.warpAffine(train, M, (width, height))  # Rotate\n",
    "query = query[500:2000, 500:3000]  # Crop\n",
    "\n",
    "# Save grayscale version\n",
    "train_gray = cv.cvtColor(train, cv.COLOR_RGB2GRAY)\n",
    "query_gray = cv.cvtColor(query, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "axs[0].imshow(train)\n",
    "axs[1].imshow(query);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the naming convention of a `train` and `query` image for this notebook. The `train` image is a complex scene which includes the `query` image. Hence, for our pipeline the digital image would be the `train` image while the film equivalent would be the `query` image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Nice, we have our dummy images. First, we have to find relevant keypoints in the images that we will later match on. We will start by exploring different methods to find the keypoints and descriptors of the images.\n",
    "\n",
    "* **SIFT** (Scale-Invariant Feature Transform) [[Paper](), [Wikipedia](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform), [Tutorial](https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html)]\n",
    "* **ORB** (Oriented FAST and Rotated BRIEF) [[Paper](), [Wikipedia](), [Tutorial](https://docs.opencv.org/4.x/d1/d89/tutorial_py_orb.html)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SIFT Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SIFT object and detect keypoints\n",
    "sift = cv.SIFT_create()\n",
    "kp = sift.detect(train_gray, None)\n",
    "\n",
    "# Draw image w/ keypoints\n",
    "plt.imshow(cv.drawKeypoints(train_gray, kp, None, color=(0, 255, 0)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ORB Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate ORB detector\n",
    "orb = cv.ORB_create()\n",
    "kp = orb.detect(train_gray, None)\n",
    "\n",
    "# Draw image w/ keypoints\n",
    "plt.imshow(cv.drawKeypoints(train_gray, kp, None, color=(0, 255, 0)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will return the keypoints and descriptors of an image for different methods and with the option of only searching over an area in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "    img: np.ndarray, method: str = \"orb\", mask: Union[np.ndarray, None] = None, **kwargs\n",
    ") -> Tuple[List[cv.KeyPoint], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract features from an image using a given method.\n",
    "    Currently supports SIFT and ORB.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Image to get features\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[cv.KeyPoint], np.ndarray]: Tuple containing the keypoints and descriptors\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale\n",
    "    gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Create the extractor\n",
    "    if method == \"sift\":\n",
    "        extractor = cv.SIFT_create(**kwargs)\n",
    "    elif method == \"orb\":\n",
    "        extractor = cv.ORB_create(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method\")\n",
    "\n",
    "    # Extract features\n",
    "    kp, des = extractor.detectAndCompute(gray, mask)\n",
    "\n",
    "    return kp, des"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, let's try to align two match the features between the original image and its crop. We will start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keypoints and descriptors from images\n",
    "train_kp, train_ds = extract_features(train, method=\"orb\")\n",
    "\n",
    "# Plot\n",
    "_, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "axs[0].imshow(train)\n",
    "axs[1].imshow(\n",
    "    cv.drawKeypoints(cv.cvtColor(train, cv.COLOR_RGB2GRAY), train_kp, None, color=(0, 255, 0))\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Found {len(train_kp)} keypoints in original image (each descriptor has {train_ds.shape[1]} features)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keypoints and descriptors from images\n",
    "query_kp, query_ds = extract_features(query, method=\"orb\")\n",
    "\n",
    "# Plot\n",
    "_, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "axs[0].imshow(query)\n",
    "axs[1].imshow(\n",
    "    cv.drawKeypoints(cv.cvtColor(query, cv.COLOR_RGB2GRAY), query_kp, None, color=(0, 255, 0))\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Found {len(query_kp)} keypoints in original image (each descriptor has {query_ds.shape[1]} features)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, this worked - however we can already see that it is not likely that we have an overlap in keypoints between the train and query image because ORB detects the keypoints in different regions of the image. For now, we will ignore this and continue with the matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching\n",
    "\n",
    "or each image we have $n$ **keypoints**, each descriped by  in $d$ dimensions and we wish to find \"high-quality\" matches. OpenCV defines two matching algorithms:\n",
    "\n",
    "* **Brute-Force Matcher** [[Tutorial]()]\n",
    "* **FLANN Matcher** [[Tutorial]()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brute-Force Matcher**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise brute-force matcher\n",
    "bf = cv.BFMatcher()\n",
    "\n",
    "# Match the descriptors\n",
    "matches = bf.knnMatch(query_ds, train_ds, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append([m])\n",
    "matches = good\n",
    "\n",
    "# Display the best matching points\n",
    "_, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.imshow(cv.drawMatchesKnn(query, query_kp, train, train_kp, good, None, flags=2))\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FLANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise FLANN matcher\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "FLANN_INDEX_LSH = 6\n",
    "index_params = dict(algorithm=FLANN_INDEX_LSH, table_number=6, key_size=12, multi_probe_level=1)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "flann = cv.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Match the descriptors\n",
    "matches = flann.knnMatch(query_ds, train_ds, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.5 * n.distance:\n",
    "        good.append([m])\n",
    "matches = good\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0, 0] for i in range(len(matches))]\n",
    "\n",
    "draw_params = dict(\n",
    "    matchColor=(0, 255, 0), singlePointColor=(255, 0, 0), matchesMask=matchesMask, flags=0\n",
    ")\n",
    "\n",
    "# Display the best matching points\n",
    "_, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.imshow(cv.drawMatchesKnn(query, query_kp, train, train_kp, good, None, flags=2))\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default matching with default parameters is not very good. We will define another helper function to match features and then one than combines the feature extraction and matching, so that we can experiment with both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(query_ds, train_ds, method=\"flann\", **kwargs):\n",
    "    \"\"\"\n",
    "    Match features between two sets of keypoints and descriptors.\n",
    "    Currently supports brute-force and FLANN.\n",
    "\n",
    "    Args:\n",
    "        query_ds (np.ndarray): Descriptors of query image\n",
    "        train_ds (np.ndarray): Descriptors of train image\n",
    "        method (str, optional): Matching method. Defaults to \"bf\".\n",
    "        **kwargs: Additional arguments for the matcher\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[cv.KeyPoint], np.ndarray, List[cv.KeyPoint], np.ndarray, List[cv.DMatch]]: Tuple containing the keypoints and descriptors\n",
    "    \"\"\"\n",
    "    # Create the matcher\n",
    "    if method == \"bf\":\n",
    "        matcher = cv.BFMatcher(**kwargs)\n",
    "    elif method == \"flann\":\n",
    "        matcher = cv.FlannBasedMatcher(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method\")\n",
    "\n",
    "    # Match the descriptors\n",
    "    matches = matcher.knnMatch(query_ds, train_ds, k=2)\n",
    "\n",
    "    # Apply ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append([m])\n",
    "\n",
    "    return good_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment with various configurations of the ORB feature detector and brute-force matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "query_kp, query_ds = extract_features(query)\n",
    "train_kp, train_ds = extract_features(train)\n",
    "orb_flann_kwargs = dict(\n",
    "    indexParams=dict(algorithm=6, table_number=6, key_size=12, multi_probe_level=1),\n",
    "    searchParams=dict(checks=50),\n",
    ")\n",
    "matches = match_features(query_ds, train_ds, method=\"flann\", **orb_flann_kwargs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.imshow(cv.drawMatchesKnn(query, query_kp, train, train_kp, matches, None, flags=2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad. Let's try to align the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment\n",
    "\n",
    "Finally, we want to align the two images based on the matched keypoints. To do so, we estimate a **homography** matrix which we use to transform that train (large) image in a way that we can crop into the detected zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find keypoints\n",
    "query_pts = np.float32([query_kp[m[0].queryIdx].pt for m in matches]).reshape(-1, 1, 2)  # query\n",
    "train_pts = np.float32([train_kp[m[0].trainIdx].pt for m in matches]).reshape(-1, 1, 2)  # train\n",
    "\n",
    "# Estimate homography matrix\n",
    "M, mask = cv.findHomography(train_pts, query_pts, cv.RANSAC, 5.0)\n",
    "matchesMask = mask.ravel().tolist()\n",
    "\n",
    "# Find the perspective transformation\n",
    "h, w = query_gray.shape  # query\n",
    "pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "dst = cv.perspectiveTransform(pts, M)\n",
    "\n",
    "# Align train image\n",
    "aligned_train = cv.warpPerspective(train, M, (w, h))[0:h, 0:w]\n",
    "\n",
    "_, axs = plt.subplots(ncols=3, figsize=(15, 10))\n",
    "axs[0].imshow(train)\n",
    "axs[1].imshow(query)\n",
    "axs[2].imshow(aligned_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine all of the above steps (feature extraction, matching and alignment) into a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(\n",
    "    query: np.ndarray, train: np.ndarray, query_kp: List, train_kp: List, matches: List\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Aligns two images using a homography matrix estimated from keypoint matches.\n",
    "\n",
    "    Args:\n",
    "        query (np.ndarray): The query image\n",
    "        train (np.ndarray): The train image\n",
    "        query_kp (List): Keypoints of the query image\n",
    "        train_kp (List): Keypoints of the train image\n",
    "        matches (List): List of matches\n",
    "\n",
    "    Returns:\n",
    "        Transformed train image\n",
    "    \"\"\"\n",
    "    # Find keypoints\n",
    "    query_pts = np.float32([query_kp[m[0].queryIdx].pt for m in matches]).reshape(\n",
    "        -1, 1, 2\n",
    "    )  # query\n",
    "    train_pts = np.float32([train_kp[m[0].trainIdx].pt for m in matches]).reshape(\n",
    "        -1, 1, 2\n",
    "    )  # train\n",
    "\n",
    "    # Estimate homography matrix\n",
    "    M, _ = cv.findHomography(train_pts, query_pts, cv.RANSAC, 5.0)\n",
    "\n",
    "    # Find the perspective transformation\n",
    "    h, w = query.shape[:2]  # query\n",
    "    transformed_train = cv.warpPerspective(train, M, (w, h))[0:h, 0:w]\n",
    "\n",
    "    return transformed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_images(\n",
    "    query: np.ndarray,\n",
    "    train: np.ndarray,\n",
    "    mask: Union[np.ndarray, None] = None,\n",
    "    extract_method: str = \"orb\",\n",
    "    match_method: str = \"bf\",\n",
    "    extract_kwargs: dict = {},\n",
    "    match_kwargs: dict = {},\n",
    "    transform_kwargs: dict = {},\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Aligns the train image (complex scence, expected to include the query)\n",
    "    image using a pipeline of feature extraction, matching and homography.\n",
    "\n",
    "    For our dataset, the train image is the digital image and the query image\n",
    "    is the film image.\n",
    "\n",
    "    Args:\n",
    "        query (np.ndarray): Query image\n",
    "        train (np.ndarray): Train image\n",
    "        mask (np.ndarray, optional): Mask for feature extraction\n",
    "        extract_method (str, optional): Feature extraction method. Defaults to \"orb\".\n",
    "        match_method (str, optional): Feature matching method. Defaults to \"bf\".\n",
    "        extract_kwargs (dict, optional): Additional arguments for feature extraction. Defaults to {}.\n",
    "        match_kwargs (dict, optional): Additional arguments for feature matching. Defaults to {}.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of aligned images (query, aligned_train)\n",
    "    \"\"\"\n",
    "    # Extract features\n",
    "    query_kp, query_ds = extract_features(query, method=extract_method, **extract_kwargs)\n",
    "    train_kp, train_ds = extract_features(\n",
    "        train, method=extract_method, mask=mask, **extract_kwargs\n",
    "    )\n",
    "\n",
    "    # Match features\n",
    "    matches = match_features(query_ds, train_ds, method=match_method, **match_kwargs)\n",
    "\n",
    "    # Align images\n",
    "    aligned_train = transform_image(query, train, query_kp, train_kp, matches, **transform_kwargs)\n",
    "\n",
    "    return query, aligned_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align images\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 10))\n",
    "aligned_train, query = align_images(\n",
    "    query=query,\n",
    "    train=train,\n",
    "    extract_method=\"orb\",\n",
    "    match_method=\"bf\",\n",
    ")\n",
    "\n",
    "axs[0].imshow(aligned_train)\n",
    "axs[1].imshow(query);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align images\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 10))\n",
    "aligned_train, query = align_images(\n",
    "    query=query,\n",
    "    train=train,\n",
    "    extract_method=\"orb\",\n",
    "    match_method=\"flann\",\n",
    "    match_kwargs=orb_flann_kwargs,\n",
    ")\n",
    "\n",
    "axs[0].imshow(aligned_train)\n",
    "axs[1].imshow(query);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that all combinations of feature extraction and matching methods are able to align the images. However, the example is also relatively simple, as the query image is a rotated crop of the train image. We will have to use our dataset of image pairs to adjust the hyper-parameter values and find the best combination of methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment (Actual Data)\n",
    "\n",
    "---\n",
    "\n",
    "Nice, next let's test out the alignment pipeline on our dataset. We will use the `load_image_pair` and `align_images` utility functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.load import load_image_pair\n",
    "\n",
    "# Load example image\n",
    "film, digital, meta = load_image_pair(13, processing_state=\"raw\", as_array=True)\n",
    "\n",
    "print(f\"Digital: {digital.shape}, Film: {film.shape}\")\n",
    "_, axs = plt.subplots(ncols=2, figsize=(15, 10))\n",
    "axs[0].imshow(digital)\n",
    "axs[0].set_title(\"Digital\")\n",
    "axs[1].imshow(film)\n",
    "axs[1].set_title(\"Film\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keypoints and descriptors from images\n",
    "digital_kp, digital_ds = extract_features(digital, method=\"orb\")\n",
    "film_kp, film_ds = extract_features(film, method=\"orb\")\n",
    "\n",
    "# Plot\n",
    "_, axs = plt.subplots(nrows=2, ncols=2, figsize=(30, 20))\n",
    "axs[0, 0].imshow(digital)\n",
    "axs[0, 1].imshow(\n",
    "    cv.drawKeypoints(cv.cvtColor(digital, cv.COLOR_RGB2GRAY), digital_kp, None, color=(0, 255, 0))\n",
    ")\n",
    "axs[1, 0].imshow(film)\n",
    "axs[1, 1].imshow(\n",
    "    cv.drawKeypoints(cv.cvtColor(film, cv.COLOR_RGB2GRAY), film_kp, None, color=(0, 255, 0))\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Found {len(digital_kp)} keypoints in digital image (each descriptor has {digital_ds.shape[1]} features)\"\n",
    ")\n",
    "print(\n",
    "    f\"Found {len(film_kp)} keypoints in film image (each descriptor has {film_ds.shape[1]} features)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the descriptors\n",
    "orb_flann_kwargs = dict(\n",
    "    indexParams=dict(algorithm=6, table_number=6, key_size=12, multi_probe_level=1),\n",
    "    searchParams=dict(checks=50),\n",
    ")\n",
    "matches = match_features(film_ds, digital_ds, method=\"flann\", **orb_flann_kwargs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.imshow(cv.drawMatchesKnn(film, film_kp, digital, digital_kp, matches, None, flags=2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform train image (digital)\n",
    "aligned_digital = transform_image(film, digital, film_kp, digital_kp, matches)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.imshow(aligned_digital);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.preprocess import keypoint_align\n",
    "\n",
    "# Align images\n",
    "aligned_film, aligned_digital = keypoint_align(\n",
    "    query=film,\n",
    "    train=digital,\n",
    "    extract_method=\"orb\",\n",
    "    match_method=\"bf\",\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 10))\n",
    "axs[0].imshow(aligned_film)\n",
    "axs[1].imshow(aligned_digital);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.load import load_metadata\n",
    "\n",
    "# Load metadata\n",
    "meta = load_metadata()\n",
    "\n",
    "# Get all image indices\n",
    "image_indices = list(meta.keys())\n",
    "\n",
    "print(f\"There are {len(meta)} images in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align all images\n",
    "for i, idx in enumerate(image_indices):\n",
    "    # Load image pair (digital and film)\n",
    "    film, digital, _ = load_image_pair(idx, processing_state=\"raw\", as_array=True)\n",
    "\n",
    "    # Initialise figure\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "    fig.suptitle(f\"Image Pair {idx}\", fontsize=16)\n",
    "\n",
    "    try:\n",
    "        # Align images\n",
    "        _, aligned_digital = keypoint_align(\n",
    "            query=film,\n",
    "            train=digital,\n",
    "            extract_method=\"sift\",\n",
    "            match_method=\"flann\",\n",
    "        )\n",
    "        axs[0].imshow(aligned_digital)\n",
    "        axs[0].set_title(\"Aligned Digital Image\")\n",
    "    except Exception as e:\n",
    "        axs[0].imshow(np.zeros_like(digital))\n",
    "        axs[0].set_title(f\"Failed to Align Digital Image ({e})\")\n",
    "\n",
    "    axs[1].imshow(film)\n",
    "    axs[1].set_title(\"Film Image\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different configuration for failed alignment\n",
    "failed_indices = [9, 11, 40]  # Fails with SIFT + FLANN\n",
    "\n",
    "for i, idx in enumerate(failed_indices):\n",
    "    # Load image pair (digital and film)\n",
    "    film, digital, _ = load_image_pair(idx, processing_state=\"raw\", as_array=True)\n",
    "\n",
    "    # Initialise figure\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "    fig.suptitle(f\"Image Pair {idx}\", fontsize=16)\n",
    "\n",
    "    try:\n",
    "        # Align images\n",
    "        _, aligned_digital = align_images(\n",
    "            query=film,\n",
    "            train=digital,\n",
    "            extract_method=\"sift\",\n",
    "            match_method=\"flann\",\n",
    "            extract_kwargs=dict(nfeatures=1000),\n",
    "            match_kwargs=dict(\n",
    "                indexParams=dict(algorithm=1, trees=10), searchParams=dict(checks=100)\n",
    "            ),\n",
    "        )\n",
    "        axs[0].imshow(aligned_digital)\n",
    "        axs[0].set_title(\"Aligned Digital Image\")\n",
    "    except Exception as e:\n",
    "        axs[0].imshow(np.zeros_like(digital))\n",
    "        axs[0].set_title(f\"Failed to Align Digital Image ({e})\")\n",
    "\n",
    "    axs[1].imshow(film)\n",
    "    axs[1].set_title(\"Film Image\")\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sillystill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
