{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to Addition of Blue Tint and Noise\n",
    "\n",
    "In this notebook, we get evaluation metrics for baseline methods. This being\n",
    "\n",
    "- No alteration of the digital at all\n",
    "- applying a blue filter\n",
    "- applying Gaussian noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.image import (\n",
    "    StructuralSimilarityIndexMeasure as SSIM,\n",
    "    PeakSignalNoiseRatio as PSNR,\n",
    ")\n",
    "\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity as LPIPS\n",
    "from src.eval import PieAPP\n",
    "from src.data.components import PairedDataset\n",
    "import torchvision.transforms.v2 as T\n",
    "from src.models import transforms as CT\n",
    "from src.utils.utils import process_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricCollection(\n",
    "    {\n",
    "        \"ssim\": SSIM(),\n",
    "        \"psnr\": PSNR(),\n",
    "        \"lpips\": LPIPS(),\n",
    "        \"pieapp\": PieAPP(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "RAW_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(RAW_DIR, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Alteration\n",
    "\n",
    "We compute metrics on simply predicting the film image as the digital image. I.e. we compute metrics over the (digital, film) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_paired_dir = os.path.join(DATA_DIR, \"paired\", \"processed\", \"film\")\n",
    "digital_paired_dir = os.path.join(DATA_DIR, \"paired\", \"processed\", \"digital\")\n",
    "digital_film_data = PairedDataset(image_dirs=(film_paired_dir, digital_paired_dir))\n",
    "film_0, digital_0 = digital_film_data[0]\n",
    "digital_film_subset = Subset(digital_film_data, range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first example and compute its metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_0_processed, digital_0_processed = process_pair(film_0, digital_0)\n",
    "print(film_0_processed.shape, digital_0_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_0 = metrics(film_0_processed.unsqueeze(0), digital_0_processed.unsqueeze(0))\n",
    "metrics_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now iterate over all the images the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {}\n",
    "for film, digital in tqdm(digital_film_subset):\n",
    "    film, digital = process_pair(film, digital)\n",
    "    film, digital = film.unsqueeze(0), digital.unsqueeze(0)\n",
    "\n",
    "    for metric in metrics:\n",
    "        if metric not in all_metrics:\n",
    "            all_metrics[metric] = []\n",
    "        \n",
    "        score = metrics[metric](film, digital)\n",
    "\n",
    "        if isinstance(score, torch.Tensor):\n",
    "            score = score.item()\n",
    "\n",
    "        all_metrics[metric].append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add a blue filter, Guassian blur, and a combination of the two and see what the test metrics look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grain(nn.Module):\n",
    "    def __init__(self, grain_sizes, intensities):\n",
    "        \"\"\"\n",
    "        Initialize the GrainTransform with specified grain sizes and their respective intensities.\n",
    "        \n",
    "        Args:\n",
    "            grain_sizes (list of int): List of grain sizes to be applied.\n",
    "            intensities (list of float): List of intensities corresponding to each grain size.\n",
    "                                         Should be between 0 (no grain) and 1 (full grain).\n",
    "        \"\"\"\n",
    "        super(Grain, self).__init__()\n",
    "        assert len(grain_sizes) == len(intensities), \"Grain sizes and intensities lists must be of the same length\"\n",
    "        assert all(0 <= intensity <= 1 for intensity in intensities), \"Intensities must be between 0 and 1\"\n",
    "        self.grain_sizes = grain_sizes\n",
    "        self.intensities = intensities\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Apply natural grain effect to the input image.\n",
    "        \n",
    "        Args:\n",
    "            img (Tensor): The input image tensor to be transformed. Should be of shape (B, C, H, W) or (C, H, W).\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: The transformed image tensor with grain added.\n",
    "        \"\"\"\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)  # Add batch dimension if not present\n",
    "        \n",
    "        assert img.ndimension() == 4 and img.size(1) == 3, \"Input tensor must be of shape (B, C, H, W) with 3 channels\"\n",
    "        \n",
    "        batch_size, channels, height, width = img.size()\n",
    "        grain = torch.zeros_like(img)\n",
    "        \n",
    "        for grain_size, intensity in zip(self.grain_sizes, self.intensities):\n",
    "            grain_noise = torch.randn(batch_size, 1, height // grain_size, width // grain_size, device=img.device)\n",
    "            grain_noise = F.interpolate(grain_noise, size=(height, width), mode='nearest')\n",
    "            grain += grain_noise * intensity\n",
    "        \n",
    "        img = img + grain\n",
    "        img = torch.clamp(img, 0.0, 1.0)\n",
    "        \n",
    "        return img\n",
    "\n",
    "\n",
    "class BlueTint(nn.Module):\n",
    "    def __init__(self, blue_intensity=0.5, red_shift=0.1, green_shift=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the BlueTint transform with specified intensity for blue tint and color shifts.\n",
    "        \n",
    "        Args:\n",
    "            blue_intensity (float): The intensity of the blue filter. Should be between 0 (no blue) and 1 (full blue).\n",
    "            red_shift (float): The intensity of the red shift. Should be between 0 and 1.\n",
    "            green_shift (float): The intensity of the green shift. Should be between 0 and 1.\n",
    "        \"\"\"\n",
    "        super(BlueTint, self).__init__()\n",
    "        assert 0 <= blue_intensity <= 1, \"Blue intensity must be between 0 and 1\"\n",
    "        assert 0 <= red_shift <= 1, \"Red shift intensity must be between 0 and 1\"\n",
    "        assert 0 <= green_shift <= 1, \"Green shift intensity must be between 0 and 1\"\n",
    "        \n",
    "        self.blue_intensity = blue_intensity\n",
    "        self.red_shift = red_shift\n",
    "        self.green_shift = green_shift\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Apply the BlueTint effect to the input image.\n",
    "        \n",
    "        Args:\n",
    "            img (Tensor): The input image tensor to be transformed. Should be of shape (B, C, H, W) or (C, H, W).\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: The transformed image tensor with the BlueTint effect applied.\n",
    "        \"\"\"\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)  # Add batch dimension if not present\n",
    "        \n",
    "        assert img.ndimension() == 4 and img.size(1) == 3, \"Input tensor must be of shape (B, C, H, W) with 3 channels\"\n",
    "        \n",
    "        # Apply blue filter\n",
    "        blue_filter = torch.tensor([1 - self.blue_intensity, 1 - self.blue_intensity, 1]).view(1, 3, 1, 1).to(img.device)\n",
    "        img = img * blue_filter\n",
    "\n",
    "        # Apply red shift\n",
    "        red_channel = img[:, 0, :, :] + self.red_shift\n",
    "        img[:, 0, :, :] = torch.clamp(red_channel, 0.0, 1.0)\n",
    "\n",
    "        # Apply green shift\n",
    "        green_channel = img[:, 1, :, :] + self.green_shift\n",
    "        img[:, 1, :, :] = torch.clamp(green_channel, 0.0, 1.0)\n",
    "\n",
    "        return img\n",
    "    \n",
    "\n",
    "class ColourFilter(nn.Module):\n",
    "    def __init__(self, red_intensity=1.0, green_intensity=1.0, blue_intensity=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the ColourFilter with specified intensities for red, green, and blue channels.\n",
    "        \n",
    "        Args:\n",
    "            red_intensity (float): The intensity of the red filter. Should be between 0 (no red) and 1 (full red).\n",
    "            green_intensity (float): The intensity of the green filter. Should be between 0 (no green) and 1 (full green).\n",
    "            blue_intensity (float): The intensity of the blue filter. Should be between 0 (no blue) and 1 (full blue).\n",
    "        \"\"\"\n",
    "        super(ColourFilter, self).__init__()\n",
    "        assert 0 <= red_intensity <= 1, \"Red intensity must be between 0 and 1\"\n",
    "        assert 0 <= green_intensity <= 1, \"Green intensity must be between 0 and 1\"\n",
    "        assert 0 <= blue_intensity <= 1, \"Blue intensity must be between 0 and 1\"\n",
    "        \n",
    "        self.red_intensity = red_intensity\n",
    "        self.green_intensity = green_intensity\n",
    "        self.blue_intensity = blue_intensity\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Apply the color filter to the input image.\n",
    "        \n",
    "        Args:\n",
    "            img (Tensor): The input image tensor to be transformed. Should be of shape (B, C, H, W) or (C, H, W).\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: The transformed image tensor with the color filter applied.\n",
    "        \"\"\"\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)  # Add batch dimension if not present\n",
    "        \n",
    "        assert img.ndimension() == 4 and img.size(1) == 3, \"Input tensor must be of shape (B, C, H, W) with 3 channels\"\n",
    "        \n",
    "        # Create the color filter mask\n",
    "        color_filter = torch.tensor([self.red_intensity, self.green_intensity, self.blue_intensity]).view(1, 3, 1, 1).to(img.device)\n",
    "        \n",
    "        # Apply the color filter\n",
    "        img = img * color_filter\n",
    "        \n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain = Grain(intensities=[0.02, 0.01, 0.005, 0.0025], grain_sizes=[1,2,4, 8])\n",
    "blue_tint = BlueTint(blue_intensity=0.8, red_shift=0.12, green_shift=0.12)\n",
    "blue_filter = ColourFilter(red_intensity=0.5, green_intensity=0.9, blue_intensity=1.0)\n",
    "noise_and_filter = T.Compose([grain, blue_filter])\n",
    "noise_and_tint = T.Compose([grain, blue_tint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_0_noisy = grain(digital_0_processed)\n",
    "CT.FromModelInput()(digital_0_noisy.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_0_filter = blue_filter(digital_0_processed)\n",
    "CT.FromModelInput()(digital_0_filter.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_0_noise_and_filter = noise_and_filter(digital_0_processed)\n",
    "CT.FromModelInput()(digital_0_noise_and_filter.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT.FromModelInput()(digital_0_processed.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT.FromModelInput()(film_0_processed.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compute the metrics over these transformed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_metrics = {}\n",
    "filter_metrics = {}\n",
    "noise_and_filter_metrics = {}\n",
    "\n",
    "for digital, film in tqdm(digital_film_subset):\n",
    "    digital, film = process_pair(digital, film)\n",
    "    digital, film = digital.unsqueeze(0), film.unsqueeze(0)\n",
    "    digital_noisy = grain(digital)\n",
    "    digital_filtered = blue_filter(digital)\n",
    "    digital_noise_and_filter = noise_and_filter(digital)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric not in noisy_metrics:\n",
    "            noisy_metrics[metric] = []\n",
    "            filter_metrics[metric] = []\n",
    "            noise_and_filter_metrics[metric] = []\n",
    "\n",
    "        \n",
    "        noisy_score = metrics[metric](film, digital_noisy)\n",
    "        filter_score = metrics[metric](film, digital_filtered)\n",
    "        noise_and_filter_score = metrics[metric](film, digital_noise_and_filter)\n",
    "\n",
    "        if isinstance(noisy_score, torch.Tensor):\n",
    "            noisy_score = noisy_score.item()\n",
    "            filter_score = filter_score.item()\n",
    "            noise_and_filter_score = noise_and_filter_score.item()\n",
    "\n",
    "        noisy_metrics[metric].append(noisy_score)\n",
    "        filter_metrics[metric].append(filter_score)\n",
    "        noise_and_filter_metrics[metric].append(noise_and_filter_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print means for each metric for each filter/noise combination\n",
    "for metric in metrics:\n",
    "    print(f\"{metric.upper()}:\")\n",
    "    print(f\"  Noisy: {sum(noisy_metrics[metric]) / len(noisy_metrics[metric])}\")\n",
    "    print(f\"  Filtered: {sum(filter_metrics[metric]) / len(filter_metrics[metric])}\")\n",
    "    print(f\"  Noise and Filtered: {sum(noise_and_filter_metrics[metric]) / len(noise_and_filter_metrics[metric])}\")\n",
    "    print(f\" Baseline: {sum(all_metrics[metric]) / len(all_metrics[metric])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of the metrics in the same plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axs = axs.ravel()\n",
    "metric_names = list(noisy_metrics.keys())\n",
    "for i, metric in enumerate(metric_names):\n",
    "    axs[i].hist(noisy_metrics[metric], bins=20, alpha=0.5, label='Noisy')\n",
    "    axs[i].hist(filter_metrics[metric], bins=20, alpha=0.5, label='Filtered')\n",
    "    axs[i].hist(noise_and_filter_metrics[metric], bins=20, alpha=0.5, label='Noisy and Filtered')\n",
    "    axs[i].hist(all_metrics[metric], bins=20, alpha=0.5, label='Original')\n",
    "    axs[i].set_title(metric)\n",
    "    axs[i].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sillystill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
