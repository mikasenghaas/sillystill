{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to Baselines\n",
    "\n",
    "In this notebook, we calculate evaluation metrics only comparing digital to film.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.image import (\n",
    "    StructuralSimilarityIndexMeasure as SSIM,\n",
    "    PeakSignalNoiseRatio as PSNR,\n",
    ")\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity as LPIPS\n",
    "\n",
    "from src.eval import PieAPP\n",
    "from src.models import transforms as CT\n",
    "from src.data.components import PairedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Noise Checks\n",
    "\n",
    "First, we check how calling a metric collection works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics = MetricCollection({\n",
    "    \"ssim\": SSIM(),\n",
    "    \"psnr\": PSNR(),\n",
    "    \"lpips\": LPIPS(net_type=\"squeeze\", normalize=True),\n",
    "    \"pieapp\": PieAPP(),\n",
    "})\n",
    "\n",
    "metrics = compute_metrics.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = lambda x: x.clamp(0+1e-5, 1-1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, 256, 256).to(device)\n",
    "noise = torch.rand_like(x).to(device)\n",
    "\n",
    "print(compute_metrics(c(x), c(x + noise * 0.1))) # less noise\n",
    "print(compute_metrics(c(x), c(x + noise * 0.5))) # more noise\n",
    "\n",
    "assert metrics(c(x), c(x + noise * 0.1)) == metrics(c(x), c(x + noise * 0.1)), \"Non-deterministic behavior\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the the SSIM and PSNR values are higher when adding less noise, and all other metrics are lower.\n",
    "\n",
    "We also observe that re-computing with the same inputs gives the same results, i.e. the results are deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check commutativity\n",
    "values_a = compute_metrics(c(x), c(x + noise * 0.1))\n",
    "values_b = compute_metrics(c(x + noise * 0.1), c(x))\n",
    "print(\"Is commutative\")\n",
    "for metric in metrics:\n",
    "    val_a = values_a[metric]\n",
    "    val_b = values_b[metric]\n",
    "    is_commutative = torch.allclose(val_a, val_b, atol=1e-6)\n",
    "\n",
    "    print(f\"{metric}: {is_commutative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Imge\n",
    "\n",
    "We compute metrics on a single image pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_paired_dir = os.path.join(\"data\", \"paired\", \"grain\", \"film\")\n",
    "digital_paired_dir = os.path.join(\"data\", \"paired\", \"grain\", \"digital\")\n",
    "digital_film_data = PairedDataset(image_dirs=(film_paired_dir, digital_paired_dir))\n",
    "\n",
    "print(len(digital_film_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first example and compute its metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film, digital = digital_film_data[0]\n",
    "black = Image.new('RGB', film.size)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax[0].imshow(film); ax[1].imshow(digital); ax[2].imshow(black);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to apply some transforms (to model input + resize) to be able to feed through the metrics (these are the same transforms that we do before passing the data to the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pil(img):\n",
    "    return CT.FromModelInput()(img[0].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compute some baselines scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_digital = CT.to_infer(digital, device=device, downsample=2)\n",
    "trans_film = CT.to_infer(film, device=device, downsample=2)\n",
    "trans_black = CT.to_infer(black, device=device, downsample=2)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax[0].imshow(to_pil(trans_digital))\n",
    "ax[1].imshow(to_pil(trans_film));\n",
    "ax[2].imshow(to_pil(trans_black));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(trans_digital, trans_film)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(trans_black, trans_film)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, these look like sensible numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sensitivity to downsampling\n",
    "data = []\n",
    "for factor in [2, 4, 8]:\n",
    "    trans_digital = CT.to_infer(digital, downsample=factor, device=device)\n",
    "    trans_film = CT.to_infer(film, downsample=factor, device=device)\n",
    "    metrics = compute_metrics(trans_digital, trans_film)\n",
    "\n",
    "    data.append({\"factor\": factor, **{metric: value.item() for metric, value in metrics.items()}})\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's check for sensitivity to crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sensitivity to crop\n",
    "crop = CT.TrainTransforms(256, augment=0)\n",
    "\n",
    "crop_digital = crop(trans_digital)\n",
    "crop_film = crop(trans_film)\n",
    "\n",
    "compute_metrics(crop_digital, crop_film)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on runtime:\n",
    "\n",
    "* Runtime CPU (912x1360): 23s\n",
    "* Runtime MPS (912x1360): 10s\n",
    "* Runtime MPS (1824x2728): 1.30m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_paired_dir = os.path.join(\"data\", \"paired\", \"processed\", \"film\")\n",
    "digital_paired_dir = os.path.join(\"data\", \"paired\", \"processed\", \"digital\")\n",
    "digital_film_data = PairedDataset(image_dirs=(film_paired_dir, digital_paired_dir))\n",
    "\n",
    "print(len(digital_film_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now iterate over all the images in the dataset, with a downsampling factor of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for downsample in [2, 4, 8]:\n",
    "    for i, (film, digital) in tqdm(enumerate(digital_film_data), total=len(digital_film_data), desc=f\"Downsample: {downsample}\"):\n",
    "        trans_digital = CT.to_infer(digital, downsample=downsample, device=device)\n",
    "        trans_film = CT.to_infer(film, downsample=downsample, device=devic\n",
    "                                 e)\n",
    "\n",
    "        metrics = compute_metrics(trans_digital, trans_film)\n",
    "        data.append({\n",
    "            \"image_id\": i+1,\n",
    "            \"downsample\": downsample,\n",
    "            **{metric: value.item() for metric, value in metrics.items()}\n",
    "        })\n",
    "\n",
    "baselines = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "scores = list(compute_metrics.keys())\n",
    "baselines.groupby(\"downsample\").mean()[scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation based on downsample\n",
    "print(\"Mean standard deviation of metrics across downsampling factors\")\n",
    "baselines.groupby([\"image_id\"])[scores].std().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines[baselines.downsample == 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sillystill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
