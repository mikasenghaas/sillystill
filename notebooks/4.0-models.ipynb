{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ Lightning Modules\n",
    "\n",
    "This notebook shows how to use the various model modules defined in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "Let's install some necessary dependencies and set global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoroot\n",
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.v2.functional import to_pil_image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data import PairedDigitalFilmDataModule\n",
    "from src.models.net import FFNet, ConvNet, UNet\n",
    "from src.models import TranslationModule, AutoTranslationModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseModule\n",
    "\n",
    "the `BaseModule` defines some helpful methods for transformations and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise `BaseModule`\n",
    "from src.models.base_module import BaseModule\n",
    "\n",
    "base = BaseModule(augment=0.0, training_patch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the batched data\n",
    "batch_size = 4\n",
    "data = PairedDigitalFilmDataModule(batch_size=batch_size)\n",
    "data.prepare_data(); data.setup()\n",
    "loader = data.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "film_batch, digital_batch = batch\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=batch_size, figsize=(16, 6))\n",
    "for i in range(batch_size):\n",
    "    axs[0, i].imshow(np.array(to_pil_image(film_batch[i])))\n",
    "    axs[1, i].imshow(np.array(to_pil_image(digital_batch[i])))\n",
    "axs[0, 0].set_ylabel('Film'); axs[1, 0].set_ylabel('Digital');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train transforms\n",
    "batch_train_transformed = base.train_transform(batch)\n",
    "original_batch = base.undo_transform(batch_train_transformed)\n",
    "_, digital_train_transformed = batch_train_transformed\n",
    "_, digital_original = original_batch\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=batch_size, figsize=(16, 6))\n",
    "for i in range(batch_size):\n",
    "    axs[0, i].imshow(np.array(to_pil_image(digital_train_transformed[i])))\n",
    "    axs[1, i].imshow(np.array(to_pil_image(digital_original[i])))\n",
    "axs[0, 0].set_ylabel('Train Transformation'); axs[1, 0].set_ylabel(\"Original\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test transforms\n",
    "batch_train_transformed = base.test_transform(batch)\n",
    "original_batch = base.undo_transform(batch_train_transformed)\n",
    "_, digital_train_transformed = batch_train_transformed\n",
    "_, digital_original = original_batch\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=batch_size, figsize=(16, 6))\n",
    "for i in range(batch_size):\n",
    "    axs[0, i].imshow(np.array(to_pil_image(digital_train_transformed[i])))\n",
    "    axs[1, i].imshow(np.array(to_pil_image(digital_original[i])))\n",
    "axs[0, 0].set_ylabel(\"Test Transformation\"); axs[1, 0].set_ylabel(\"Original\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TranslationModule\n",
    "\n",
    "The `TranslationModule` is a direct image-to-image translation module. It can be based on the various encoder-decoder networks (like `FFNet`, `ConvNet` or `UNet`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise `TranslationModule`\n",
    "module = TranslationModule(\n",
    "    net=FFNet(input_output_size=3*256*256),\n",
    "    loss=nn.MSELoss()\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "y = module.forward(x)\n",
    "assert x.shape == y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise `TranslationModule`\n",
    "module = TranslationModule(\n",
    "    net=ConvNet(),\n",
    "    loss=nn.MSELoss()\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "y = module.forward(x)\n",
    "assert x.shape == y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise `TranslationModule`\n",
    "module = TranslationModule(\n",
    "    net=UNet(),\n",
    "    loss=nn.MSELoss()\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "y = module.forward(x)\n",
    "assert x.shape == y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoTranslationModule\n",
    "\n",
    "Base module for auto-translation models as seen in \"Semi-Supervised\n",
    "Raw-to-Raw Mapping\": https://arxiv.org/pdf/2106.13883\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise `AutoTranslationModule`\n",
    "module = AutoTranslationModule(\n",
    "    optimizer=torch.optim.Adam,\n",
    "    scheduler=None,\n",
    ")\n",
    "\n",
    "batch = (\n",
    "    torch.randn(3, 3, 256, 256),\n",
    "    torch.randn(3, 3, 256, 256),\n",
    "    torch.randn(2, 2, 3, 256, 256),\n",
    ")\n",
    "loss, film_paired, digital_to_film = module.step(batch)\n",
    "assert loss is not None\n",
    "film_paired.shape, digital_to_film.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sillystill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
