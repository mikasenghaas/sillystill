\begin{abstract}

The use of deep learning in stylistic effect generation has seen increasing use over recent years. In this work, we use simple convolutional neural networks to model Cinestill800T film given a digital input. We test the effect of different loss functions, as well as the addition of an input noise channel and the use of random scales of patches during training. We find that a combination of MSE/VGG gives the best colour production, and that some grain can be produced, but it is not of a high quality, and no halation is produced. We contribute our dataset of aligned paired images taken with a film and digital camera for further work.

\end{abstract}